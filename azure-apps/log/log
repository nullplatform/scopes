#!/bin/bash
set -euo pipefail

# Fetch logs from Azure App Service via Kudu Docker logs API (both production and staging slots)

CACHE_DIR="${SERVICE_PATH}/log/.cache"
CACHE_TTL=3600  # 1 hour in seconds
TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

# Function to get publishing credentials (with caching)
get_credentials() {
  local slot="$1"  # "production" or "staging"
  local cache_file="${CACHE_DIR}/${APP_NAME}-${slot}-creds.json"
  local creds_url

  if [ "$slot" = "staging" ]; then
    creds_url="https://management.azure.com/subscriptions/${ARM_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP}/providers/Microsoft.Web/sites/${APP_NAME}/slots/staging/config/publishingcredentials/list?api-version=2022-03-01"
  else
    creds_url="https://management.azure.com/subscriptions/${ARM_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP}/providers/Microsoft.Web/sites/${APP_NAME}/config/publishingcredentials/list?api-version=2022-03-01"
  fi

  # Check if cached credentials exist and are fresh
  if [ -f "$cache_file" ]; then
    local cache_age=$(($(date +%s) - $(stat -c %Y "$cache_file" 2>/dev/null || stat -f %m "$cache_file" 2>/dev/null || echo 0)))
    if [ "$cache_age" -lt "$CACHE_TTL" ]; then
      cat "$cache_file"
      return 0
    fi
  fi

  # Fetch fresh credentials
  local credentials
  credentials=$(curl -s -X POST "$creds_url" \
    -H "Authorization: Bearer ${AZURE_ACCESS_TOKEN}" \
    -H "Content-Type: application/json" \
    -d '{}' 2>/dev/null)

  # Check if valid response (has publishingUserName)
  if echo "$credentials" | jq -e '.properties.publishingUserName' >/dev/null 2>&1; then
    mkdir -p "$CACHE_DIR"
    echo "$credentials" > "$cache_file"
    echo "$credentials"
    return 0
  fi

  return 1
}

# Function to fetch logs from a slot
fetch_slot_logs() {
  local slot="$1"
  local scm_user="$2"
  local scm_pass="$3"
  local scm_host="$4"
  local output_prefix="$5"

  # Fetch log file list from Kudu API
  local log_list
  log_list=$(curl -s -u "${scm_user}:${scm_pass}" "https://${scm_host}/api/logs/docker" 2>/dev/null || echo "[]")

  # Sort by lastUpdated descending and take top 3 log files
  local log_urls
  log_urls=$(echo "$log_list" | jq -r '[sort_by(.lastUpdated) | reverse | .[:3] | .[].href] | .[]' 2>/dev/null || echo "")

  # Fetch log files in parallel
  local pids=()
  local index=0
  for url in $log_urls; do
    curl -s -u "${scm_user}:${scm_pass}" "$url" > "${TEMP_DIR}/${output_prefix}_${index}" 2>/dev/null &
    pids+=($!)
    index=$((index + 1))
  done

  # Wait for all downloads
  if [ ${#pids[@]} -gt 0 ]; then
    for pid in "${pids[@]}"; do
      wait "$pid" 2>/dev/null || true
    done
  fi
}

# Fetch production slot credentials and logs
PROD_CREDS=$(get_credentials "production" || echo '{}')
if echo "$PROD_CREDS" | jq -e '.properties.publishingUserName' >/dev/null 2>&1; then
  PROD_USER=$(echo "$PROD_CREDS" | jq -r '.properties.publishingUserName')
  PROD_PASS=$(echo "$PROD_CREDS" | jq -r '.properties.publishingPassword')
  PROD_HOST="${APP_NAME}.scm.azurewebsites.net"
  fetch_slot_logs "production" "$PROD_USER" "$PROD_PASS" "$PROD_HOST" "prod_log"
fi

# Fetch staging slot credentials and logs (may not exist)
STAGING_CREDS=$(get_credentials "staging" || echo '{}')
if echo "$STAGING_CREDS" | jq -e '.properties.publishingUserName' >/dev/null 2>&1; then
  STAGING_USER=$(echo "$STAGING_CREDS" | jq -r '.properties.publishingUserName')
  STAGING_PASS=$(echo "$STAGING_CREDS" | jq -r '.properties.publishingPassword')
  STAGING_HOST="${APP_NAME}-staging.scm.azurewebsites.net"
  fetch_slot_logs "staging" "$STAGING_USER" "$STAGING_PASS" "$STAGING_HOST" "staging_log"
fi

# Combine and parse logs from all slots using jq
if ls "${TEMP_DIR}"/*_log_* >/dev/null 2>&1; then
  PARSED_LINES=$(cat "${TEMP_DIR}"/*_log_* | jq -R --arg start_time "${START_TIME:-}" --arg end_time "${END_TIME:-}" --arg filter "${FILTER_PATTERN:-}" '
    # For each line, try to extract timestamp
    select(length > 0) |
    . as $line |
    if test("^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}") then
      {
        datetime: (.[0:19] + "Z"),
        message: $line
      }
    else
      empty
    end |
    # Apply START_TIME filter (keep logs >= start_time)
    if ($start_time != "" and .datetime < $start_time) then empty else . end |
    # Apply END_TIME filter (keep logs <= end_time)
    if ($end_time != "" and .datetime > $end_time) then empty else . end |
    # Apply FILTER_PATTERN filter
    if ($filter != "" and (.message | test($filter) | not)) then empty else . end
  ' 2>/dev/null | jq -s 'sort_by(.datetime)' 2>/dev/null || echo "[]")
else
  PARSED_LINES="[]"
fi

# Apply pagination (NEXT_PAGE_TOKEN and LIMIT)
OFFSET=0
if [ -n "${NEXT_PAGE_TOKEN:-}" ]; then
  DECODED=$(echo "$NEXT_PAGE_TOKEN" | base64 -d 2>/dev/null || echo "{}")
  OFFSET=$(echo "$DECODED" | jq -r '.offset // 0')
fi

TOTAL=$(echo "$PARSED_LINES" | jq 'length')
EFFECTIVE_LIMIT=${LIMIT:-1000}

# Slice the results
RESULTS=$(echo "$PARSED_LINES" | jq --argjson offset "$OFFSET" --argjson limit "$EFFECTIVE_LIMIT" \
  '.[$offset:$offset + $limit]')

# Determine next_page_token
NEXT_OFFSET=$((OFFSET + EFFECTIVE_LIMIT))
NEXT_TOKEN=""
if [ "$NEXT_OFFSET" -lt "$TOTAL" ]; then
  NEXT_TOKEN=$(echo "{\"offset\":${NEXT_OFFSET}}" | base64 | tr -d '\n')
fi

# Output in the standard format
jq -n \
  --argjson results "$RESULTS" \
  --arg next_page_token "$NEXT_TOKEN" \
  '{results: $results, next_page_token: $next_page_token}'
