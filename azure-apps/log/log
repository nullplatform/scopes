#!/bin/bash
set -euo pipefail

# Fetch logs from Azure App Service via Kudu Docker logs API

# Get publishing credentials
CREDENTIALS=$(az webapp deployment list-publishing-credentials \
  --resource-group "$AZURE_RESOURCE_GROUP" \
  --name "$APP_NAME" \
  --output json)

SCM_USER=$(echo "$CREDENTIALS" | jq -r '.publishingUserName')
SCM_PASS=$(echo "$CREDENTIALS" | jq -r '.publishingPassword')
SCM_HOST="${APP_NAME}.scm.azurewebsites.net"

# Fetch log file list from Kudu API
LOG_LIST=$(curl -s -u "${SCM_USER}:${SCM_PASS}" "https://${SCM_HOST}/api/logs/docker")

# Sort by lastUpdated descending and take top 3 log files
LOG_URLS=$(echo "$LOG_LIST" | jq -r '[sort_by(.lastUpdated) | reverse | .[:3] | .[].href] | .[]')

# Fetch log files in parallel
TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

PIDS=()
INDEX=0
for url in $LOG_URLS; do
  curl -s -u "${SCM_USER}:${SCM_PASS}" "$url" > "${TEMP_DIR}/log_${INDEX}" &
  PIDS+=($!)
  INDEX=$((INDEX + 1))
done

# Wait for all downloads to complete
if [ ${#PIDS[@]} -gt 0 ]; then
  for pid in "${PIDS[@]}"; do
    wait "$pid" 2>/dev/null || true
  done
fi

# Combine all log content
ALL_LINES=""
for file in "${TEMP_DIR}"/log_*; do
  [ -f "$file" ] || continue
  CONTENT=$(cat "$file")
  if [ -n "$CONTENT" ]; then
    if [ -n "$ALL_LINES" ]; then
      ALL_LINES="${ALL_LINES}
${CONTENT}"
    else
      ALL_LINES="$CONTENT"
    fi
  fi
done

# Parse log lines into JSON array
# Expected format: TIMESTAMP LEVEL - MESSAGE
PARSED_LINES="[]"
if [ -n "$ALL_LINES" ]; then
  PARSED_LINES=$(echo "$ALL_LINES" | while IFS= read -r line; do
    [ -z "$line" ] && continue
    # Extract timestamp (first field before space)
    timestamp=$(echo "$line" | grep -oE '^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}' || echo "")
    if [ -n "$timestamp" ]; then
      echo "${timestamp}Z	${line}"
    fi
  done | sort | {
    echo "["
    first=true
    while IFS=$'\t' read -r ts line; do
      [ -z "$ts" ] && continue

      # Apply START_TIME filter
      if [ -n "${START_TIME:-}" ]; then
        if [[ "$ts" < "$START_TIME" ]]; then
          continue
        fi
      fi

      # Apply FILTER_PATTERN filter
      if [ -n "${FILTER_PATTERN:-}" ]; then
        if ! echo "$line" | grep -qE "$FILTER_PATTERN"; then
          continue
        fi
      fi

      if [ "$first" = true ]; then
        first=false
      else
        echo ","
      fi
      # Escape the line for JSON
      escaped_line=$(echo "$line" | jq -Rs '.')
      echo "{\"message\":${escaped_line},\"datetime\":\"${ts}\"}"
    done
    echo "]"
  })
fi

# Apply pagination (NEXT_PAGE_TOKEN and LIMIT)
OFFSET=0
if [ -n "${NEXT_PAGE_TOKEN:-}" ]; then
  DECODED=$(echo "$NEXT_PAGE_TOKEN" | base64 -d 2>/dev/null || echo "{}")
  OFFSET=$(echo "$DECODED" | jq -r '.offset // 0')
fi

TOTAL=$(echo "$PARSED_LINES" | jq 'length')
EFFECTIVE_LIMIT=${LIMIT:-1000}

# Slice the results
RESULTS=$(echo "$PARSED_LINES" | jq --argjson offset "$OFFSET" --argjson limit "$EFFECTIVE_LIMIT" \
  '.[$offset:$offset + $limit]')

# Determine next_page_token
NEXT_OFFSET=$((OFFSET + EFFECTIVE_LIMIT))
NEXT_TOKEN=""
if [ "$NEXT_OFFSET" -lt "$TOTAL" ]; then
  NEXT_TOKEN=$(echo "{\"offset\":${NEXT_OFFSET}}" | base64 | tr -d '\n')
fi

# Output in the standard format
jq -n \
  --argjson results "$RESULTS" \
  --arg next_page_token "$NEXT_TOKEN" \
  '{results: $results, next_page_token: $next_page_token}'
