#!/bin/bash
set -euo pipefail

# Timing helper (uses Python for reliable milliseconds on all platforms)
_timer() { python3 -c 'import time; print(int(time.time() * 1000))'; }
_log_timing() {
  local stage="$1" start="$2" end="$3"
  local duration=$((end - start))
  DEBUG_LOGS="${DEBUG_LOGS:-}[log] ${stage}: ${duration}ms\n"
}

START_TOTAL=$(_timer)

# Fetch logs from Azure App Service via Kudu Docker logs API

# Get publishing credentials (with caching)
START=$(_timer)
CACHE_DIR="${SERVICE_PATH}/log/.cache"
CACHE_FILE="${CACHE_DIR}/${APP_NAME}-creds.json"
CACHE_TTL=3600  # 1 hour in seconds

# Check if cached credentials exist and are fresh
USE_CACHE=false
if [ -f "$CACHE_FILE" ]; then
  CACHE_AGE=$(($(date +%s) - $(stat -c %Y "$CACHE_FILE" 2>/dev/null || stat -f %m "$CACHE_FILE" 2>/dev/null || echo 0)))
  if [ "$CACHE_AGE" -lt "$CACHE_TTL" ]; then
    USE_CACHE=true
  fi
fi

if [ "$USE_CACHE" = true ]; then
  CREDENTIALS=$(cat "$CACHE_FILE")
  _log_timing "cached_publishing_credentials" "$START" "$(_timer)"
else
  CREDS_URL="https://management.azure.com/subscriptions/${ARM_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP}/providers/Microsoft.Web/sites/${APP_NAME}/config/publishingcredentials/list?api-version=2022-03-01"
  CREDENTIALS=$(curl -s -X POST "$CREDS_URL" \
    -H "Authorization: Bearer ${AZURE_ACCESS_TOKEN}" \
    -H "Content-Type: application/json" \
    -d '{}')
  # Cache the credentials
  mkdir -p "$CACHE_DIR"
  echo "$CREDENTIALS" > "$CACHE_FILE"
  _log_timing "rest_publishing_credentials" "$START" "$(_timer)"
fi

SCM_USER=$(echo "$CREDENTIALS" | jq -r '.properties.publishingUserName')
SCM_PASS=$(echo "$CREDENTIALS" | jq -r '.properties.publishingPassword')
SCM_HOST="${APP_NAME}.scm.azurewebsites.net"

# Fetch log file list from Kudu API
START=$(_timer)
LOG_LIST=$(curl -s -u "${SCM_USER}:${SCM_PASS}" "https://${SCM_HOST}/api/logs/docker")
_log_timing "curl_log_list" "$START" "$(_timer)"

# Sort by lastUpdated descending and take top 3 log files
LOG_URLS=$(echo "$LOG_LIST" | jq -r '[sort_by(.lastUpdated) | reverse | .[:3] | .[].href] | .[]')

# Fetch log files in parallel
TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

START=$(_timer)
PIDS=()
INDEX=0
for url in $LOG_URLS; do
  curl -s -u "${SCM_USER}:${SCM_PASS}" "$url" > "${TEMP_DIR}/log_${INDEX}" &
  PIDS+=($!)
  INDEX=$((INDEX + 1))
done

# Wait for all downloads to complete
if [ ${#PIDS[@]} -gt 0 ]; then
  for pid in "${PIDS[@]}"; do
    wait "$pid" 2>/dev/null || true
  done
fi
_log_timing "curl_log_files_parallel(${INDEX})" "$START" "$(_timer)"

# Combine and parse logs using jq (much faster than bash loops)
START=$(_timer)
if ls "${TEMP_DIR}"/log_* >/dev/null 2>&1; then
  PARSED_LINES=$(cat "${TEMP_DIR}"/log_* | jq -R --arg start_time "${START_TIME:-}" --arg end_time "${END_TIME:-}" --arg filter "${FILTER_PATTERN:-}" '
    # For each line, try to extract timestamp
    select(length > 0) |
    . as $line |
    if test("^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}") then
      {
        datetime: (.[0:19] + "Z"),
        message: $line
      }
    else
      empty
    end |
    # Apply START_TIME filter (keep logs >= start_time)
    if ($start_time != "" and .datetime < $start_time) then empty else . end |
    # Apply END_TIME filter (keep logs <= end_time)
    if ($end_time != "" and .datetime > $end_time) then empty else . end |
    # Apply FILTER_PATTERN filter
    if ($filter != "" and (.message | test($filter) | not)) then empty else . end
  ' 2>/dev/null | jq -s 'sort_by(.datetime)' 2>/dev/null || echo "[]")
else
  PARSED_LINES="[]"
fi
_log_timing "parse_logs" "$START" "$(_timer)"

# Apply pagination (NEXT_PAGE_TOKEN and LIMIT)
OFFSET=0
if [ -n "${NEXT_PAGE_TOKEN:-}" ]; then
  DECODED=$(echo "$NEXT_PAGE_TOKEN" | base64 -d 2>/dev/null || echo "{}")
  OFFSET=$(echo "$DECODED" | jq -r '.offset // 0')
fi

TOTAL=$(echo "$PARSED_LINES" | jq 'length')
EFFECTIVE_LIMIT=${LIMIT:-1000}

# Slice the results
RESULTS=$(echo "$PARSED_LINES" | jq --argjson offset "$OFFSET" --argjson limit "$EFFECTIVE_LIMIT" \
  '.[$offset:$offset + $limit]')

# Determine next_page_token
NEXT_OFFSET=$((OFFSET + EFFECTIVE_LIMIT))
NEXT_TOKEN=""
if [ "$NEXT_OFFSET" -lt "$TOTAL" ]; then
  NEXT_TOKEN=$(echo "{\"offset\":${NEXT_OFFSET}}" | base64 | tr -d '\n')
fi

_log_timing "TOTAL_log" "$START_TOTAL" "$(_timer)"

# Prepend debug timing logs if not disabled
if [ "${DISABLE_DEBUG_LOGS:-}" != "true" ]; then
  NOW=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  DEBUG_ENTRIES=$(echo -e "$DEBUG_LOGS" | while IFS= read -r line; do
    [ -z "$line" ] && continue
    escaped=$(echo "$line" | jq -Rs '.')
    echo "{\"message\":${escaped},\"datetime\":\"${NOW}\"}"
  done | jq -s '.')

  # Prepend debug entries to results
  RESULTS=$(jq -n --argjson debug "$DEBUG_ENTRIES" --argjson results "$RESULTS" '$debug + $results')
fi

# Output in the standard format
jq -n \
  --argjson results "$RESULTS" \
  --arg next_page_token "$NEXT_TOKEN" \
  '{results: $results, next_page_token: $next_page_token}'
